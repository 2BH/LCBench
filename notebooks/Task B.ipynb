{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B: Meta-Learning Perfomance Prediction\n",
    "\n",
    "In this task, you will use information on training parameters and metadata on multiple OpenML dataset to train a performance predictor that performs well even for unseen datasets. You are provided with config parameters and metafeatures for six datasets. The datasets are split into training datasets and test datasets and you should only train on the training datasets.\n",
    "\n",
    "__Note: Please use the dataloading and splits you are provided with in this notebook.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifications:\n",
    "\n",
    "* Data: six_datasets_lw.json\n",
    "* Number of datasets: 6\n",
    "* Training datasets: higgs, vehicle, adult, volkert\n",
    "* Test datasets: Fashion-MNIST, jasmine\n",
    "* Number of configurations: 2000\n",
    "* Number of epochs seen when predicting: 10\n",
    "* Available data: Architecture parameters and hyperparameters, metafeatures \n",
    "* Target: Final validation accuracy\n",
    "* Evaluation metric: MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and splitting data\n",
    "\n",
    "Note: There are 51 steps logged, 50 epochs plus the 0th epoch, prior to any weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd ..\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from api import Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading data...\n",
      "==> No cached data found or cache set to False.\n",
      "==> Reading json data...\n",
      "==> Done.\n"
     ]
    }
   ],
   "source": [
    "bench_dir = \"cached/six_datasets_lw.json\"\n",
    "bench = Benchmark(bench_dir, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cached/metafeatures.json\", \"r\") as f:\n",
    "    metafeatures = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fashion-MNIST', 'adult', 'higgs', 'jasmine', 'vehicle', 'volkert']\n"
     ]
    }
   ],
   "source": [
    "# Dataset split\n",
    "dataset_names = bench.get_dataset_names()\n",
    "print(dataset_names)\n",
    "\n",
    "train_datasets = ['adult', 'higgs', 'vehicle', 'volkert']\n",
    "test_datasets = ['Fashion-MNIST', 'jasmine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (6000,)\n",
      "X_test: (4000,)\n",
      "X_val: (2000,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "def read_data(datasets):\n",
    "    n_configs = bench.get_number_of_configs(datasets[0])\n",
    "    data = [bench.query(dataset_name=d, tag=\"Train/val_accuracy\", config_id=ind) for d in datasets for ind in range(n_configs)]\n",
    "    configs = [bench.query(dataset_name=d, tag=\"config\", config_id=ind) for d in datasets for ind in range(n_configs)]\n",
    "    dataset_names = [d for d in datasets for ind in range(n_configs)]\n",
    "    \n",
    "    y = np.array([curve[-1] for curve in data])\n",
    "    return np.array(configs), y, np.array(dataset_names)\n",
    "\n",
    "class TrainValSplitter():\n",
    "    \"\"\"Splits 25 % data as a validation split.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_names):\n",
    "        self.ind_train, self.ind_val = train_test_split(np.arange(len(X)), test_size=0.25, stratify=dataset_names)\n",
    "        \n",
    "    def split(self, a):\n",
    "        return a[self.ind_train], a[self.ind_val]\n",
    "\n",
    "X, y, dataset_names = read_data(train_datasets)\n",
    "X_test, y_test, dataset_names_test = read_data(test_datasets)\n",
    "\n",
    "tv_splitter = TrainValSplitter(dataset_names=dataset_names)\n",
    "\n",
    "X_train, X_val = tv_splitter.split(X)\n",
    "y_train, y_val = tv_splitter.split(y)\n",
    "dataset_names_train, dataset_names_val = tv_splitter.split(dataset_names)\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"X_val:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains the configuration.\n",
    "\n",
    "__Note__: Not all parameters vary across different configurations. The varying parameters are batch_size, max_dropout, max_units, num_layers, learning_rate, momentum, weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config example: {'batch_size': 110, 'imputation_strategy': 'mean', 'learning_rate_scheduler': 'cosine_annealing', 'loss': 'cross_entropy_weighted', 'network': 'shapedmlpnet', 'max_dropout': 0.23231503590300873, 'normalization_strategy': 'standardize', 'optimizer': 'sgd', 'cosine_annealing_T_max': 50, 'cosine_annealing_eta_min': 1e-08, 'activation': 'relu', 'max_units': 111, 'mlp_shape': 'funnel', 'num_layers': 3, 'learning_rate': 0.0039223402512093665, 'momentum': 0.13132685322946197, 'weight_decay': 0.05408607394210568}\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "print(\"Config example:\", X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoCorrelation : 0.634425994553756\n",
      "ClassEntropy : 0.7938438393644257\n",
      "Dimensionality : 0.00030711273084640267\n",
      "EquivalentNumberOfAtts : 11.068507517484338\n",
      "MajorityClassPercentage : 76.07182343065395\n",
      "MajorityClassSize : 37155.0\n",
      "MaxAttributeEntropy : 3.44192266924963\n",
      "MaxKurtosisOfNumericAtts : 152.69309629815925\n",
      "MaxMeansOfNumericAtts : 189664.13459727284\n",
      "MaxMutualInformation : 0.16542318099233\n",
      "MaxNominalAttDistinctValues : 41.0\n",
      "MaxSkewnessOfNumericAtts : 11.89465899659272\n",
      "MaxStdDevOfNumericAtts : 105604.02542315713\n",
      "MeanAttributeEntropy : 1.7809891200338273\n",
      "MeanKurtosisOfNumericAtts : 30.359637681213712\n",
      "MeanMeansOfNumericAtts : 31819.974765570616\n",
      "MeanMutualInformation : 0.0717209468494675\n",
      "MeanNoiseToSignalRatio : 23.832203118732952\n",
      "MeanNominalAttDistinctValues : 11.222222222222221\n",
      "MeanSkewnessOfNumericAtts : 3.063860808334838\n",
      "MeanStdDevOfNumericAtts : 18914.620326608216\n",
      "MinAttributeEntropy : 0.795215031650176\n",
      "MinKurtosisOfNumericAtts : -0.18426874062378573\n",
      "MinMeansOfNumericAtts : 10.078088530363212\n",
      "MinMutualInformation : 0.00818704228545\n",
      "MinNominalAttDistinctValues : 2.0\n",
      "MinSkewnessOfNumericAtts : -0.31652485666094055\n",
      "MinStdDevOfNumericAtts : 2.5709727555918307\n",
      "MinorityClassPercentage : 23.928176569346054\n",
      "MinorityClassSize : 11687.0\n",
      "NumberOfBinaryFeatures : 2.0\n",
      "NumberOfClasses : 2.0\n",
      "NumberOfFeatures : 15.0\n",
      "NumberOfInstances : 48842.0\n",
      "NumberOfInstancesWithMissingValues : 3620.0\n",
      "NumberOfMissingValues : 6465.0\n",
      "NumberOfNumericFeatures : 6.0\n",
      "NumberOfSymbolicFeatures : 9.0\n",
      "PercentageOfBinaryFeatures : 13.333333333333334\n",
      "PercentageOfInstancesWithMissingValues : 7.411653904426519\n",
      "PercentageOfMissingValues : 0.8824372466319971\n",
      "PercentageOfNumericFeatures : 40.0\n",
      "PercentageOfSymbolicFeatures : 60.0\n",
      "Quartile1AttributeEntropy : 0.8343198263526672\n",
      "Quartile1KurtosisOfNumericAtts : 0.42324176943424885\n",
      "Quartile1MeansOfNumericAtts : 31.502211211662093\n",
      "Quartile1MutualInformation : 0.01068033823523\n",
      "Quartile1SkewnessOfNumericAtts : 0.09993102873665205\n",
      "Quartile1StdDevOfNumericAtts : 9.936326207089905\n",
      "Quartile2AttributeEntropy : 1.6008102873649352\n",
      "Quartile2KurtosisOfNumericAtts : 4.504453651154136\n",
      "Quartile2MeansOfNumericAtts : 63.96234797919819\n",
      "Quartile2MutualInformation : 0.0623734340938\n",
      "Quartile2SkewnessOfNumericAtts : 0.9982360975676194\n",
      "Quartile2StdDevOfNumericAtts : 208.3575310294492\n",
      "Quartile3AttributeEntropy : 2.736773252802121\n",
      "Quartile3KurtosisOfNumericAtts : 53.18403354052852\n",
      "Quartile3MeansOfNumericAtts : 48225.334368985714\n",
      "Quartile3MutualInformation : 0.14076964990508\n",
      "Quartile3SkewnessOfNumericAtts : 6.4010213924528\n",
      "Quartile3StdDevOfNumericAtts : 31990.020649029368\n",
      "StdvNominalAttDistinctValues : 12.152960316089429\n"
     ]
    }
   ],
   "source": [
    "# Metafeatures\n",
    "example_dataset = dataset_names[0]\n",
    "for key,val in metafeatures[example_dataset].items():\n",
    "    print(\" : \".join([key, str(val)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantPerformancePredictor():\n",
    "    \"\"\"A learning curve predictor that predicts the last observed epoch as final performance\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.constant_prediction = 0\n",
    "        \n",
    "    def fit(self, X, y, dataset_names, metafeatures):\n",
    "        self.constant_prediction = np.mean(y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = [self.constant_prediction] * len(X)\n",
    "        return predictions\n",
    "    \n",
    "def score(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282.4865842261933\n"
     ]
    }
   ],
   "source": [
    "# Train and validate\n",
    "predictor = ConstantPerformancePredictor()\n",
    "predictor.fit(X_train, y_train, dataset_names_train, metafeatures)\n",
    "preds = predictor.predict(X_val)\n",
    "mse = score(y_val, preds)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test score: 477.65356501807423\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation\n",
    "final_preds = predictor.predict(X_test)\n",
    "final_score = score(y_test, final_preds)\n",
    "print(\"Final test score:\", final_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
